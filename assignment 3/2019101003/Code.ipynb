{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56F_QMb5j4_D"
      },
      "source": [
        "## IRE ASSIGNMENT 3\n",
        "Snehal Kumar\n",
        "2019101003"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbb7YReXj_TA"
      },
      "source": [
        "## Q6. BERT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjKiJyywolaf",
        "outputId": "6766db77-4951-45ab-e347-12870b0117e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMvsO6YOmnPG"
      },
      "source": [
        "! pip install transformers -q\n",
        "! pip install tokenizers -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRvCxmN_ovSt"
      },
      "source": [
        "ROOT = \"/content/drive/MyDrive/IRE Assgn3/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtKINSr9ms9G"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, AdamW\n",
        "import tokenizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkxkclT_d9fT"
      },
      "source": [
        "def seed_all(seed = 42):\n",
        "  \"\"\"\n",
        "  Fix seed for reproducibility\n",
        "  \"\"\"\n",
        "  # python RNG\n",
        "  import random\n",
        "  random.seed(seed)\n",
        "\n",
        "  # pytorch RNGs\n",
        "  import torch\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # numpy RNG\n",
        "  import numpy as np\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otrtWaykd_-q"
      },
      "source": [
        "### Model Configs\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuZJsBDuam7O"
      },
      "source": [
        "class config:\n",
        "  SEED = 42\n",
        "  TRAIN_FILE = ROOT + 'train.tsv'\n",
        "  VAL_FILE = ROOT + 'dev.tsv'\n",
        "  TEST_FILE = ROOT + 'test.tsv'\n",
        "  SAVE_DIR = ROOT + 'outputs/'\n",
        "  MAX_LEN = 25\n",
        "  MODEL = 'bert-base-uncased'\n",
        "  # CONFIG = CONFIGFOLDER + 'finetune_bert_config.json'\n",
        "  TOKENIZER = BertTokenizerFast.from_pretrained('bert-base-uncased',lowercase=True)\n",
        "  # TOKENIZER = tokenizers.BertWordPieceTokenizer(CONFIGFOLDER + \"finetune_bert_vocab.txt\", lowercase=True)\n",
        "  EPOCHS = 10\n",
        "  TRAIN_BATCH_SIZE = 32\n",
        "  VALID_BATCH_SIZE = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUVv2WgReEjp"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2wMdNUqaSda"
      },
      "source": [
        "def process_data(file, ftype):\n",
        "    data = pd.read_csv(str(file), sep='\\t')\n",
        "    data.columns = ['text', 'rating']\n",
        "    data['rating'] = data['rating'].apply(lambda x: 1 if x >= 0.8 else 0)\n",
        "    data_text, data_rats = data['text'], data['rating']\n",
        "    tokens = config.TOKENIZER.batch_encode_plus(\n",
        "        data_text.tolist(),\n",
        "        max_length = config.MAX_LEN,\n",
        "        pad_to_max_length = True,\n",
        "        truncation = True\n",
        "    )\n",
        "    seqs = torch.tensor(tokens['input_ids'])\n",
        "    mask = torch.tensor(tokens['attention_mask'])\n",
        "    label = torch.tensor(data_rats.tolist())\n",
        "    if ftype == \"test\":\n",
        "      return seqs, mask, label\n",
        "    tensor_data = TensorDataset(seqs, mask, label)\n",
        "    if ftype == \"train\":\n",
        "      data_sampler = RandomSampler(tensor_data)\n",
        "    else:\n",
        "      data_sampler = SequentialSampler(tensor_data)\n",
        "    dataloader = DataLoader(tensor_data, sampler=data_sampler, batch_size=config.TRAIN_BATCH_SIZE)\n",
        "    data_rats = data['rating']\n",
        "    data_text = data['text']\n",
        "    return data_rats,data_text, dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd2OiP-2pI1b",
        "outputId": "a682c0e7-d795-44f2-fdae-96dbb5be5627"
      },
      "source": [
        "train_labels, train_text, train_dataloader = process_data(config.TRAIN_FILE, \"train\")\n",
        "val_labels, val_text, val_dataloader = process_data(config.VAL_FILE, \"val\")\n",
        "test_seq, test_mask, test_labels = process_data(config.TEST_FILE, \"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5h1P-76dwSD"
      },
      "source": [
        "### BERT Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntZ9mFxzdu2-"
      },
      "source": [
        "class BertDWF(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BertDWF, self).__init__()\n",
        "    self.model_config = transformers.BertConfig.from_pretrained(config.MODEL)\n",
        "    # self.bert = transformers.BertModel.from_pretrained(config.MODEL, config=config.CONFIG)\n",
        "    self.bert = transformers.BertModel.from_pretrained(config.MODEL)\n",
        "    # for param in self.bert.parameters():\n",
        "    #     param.requires_grad = False\n",
        "    self.layer1 = nn.Linear(768, 512)\n",
        "    self.layer2 = nn.Linear(512, 2)\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.relu =  nn.ReLU()\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, ids, mask):\n",
        "    _, outs = self.bert(input_ids = ids, attention_mask=mask, return_dict=False)\n",
        "\n",
        "    l1out = self.layer1(outs)\n",
        "    l1relu = self.relu(l1out)\n",
        "    l1drop = self.dropout(l1relu)\n",
        "\n",
        "    l2out = self.layer2(l1drop)\n",
        "\n",
        "    probs = self.softmax(l2out)\n",
        "\n",
        "    return probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlCDvq_pwaLZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc07e8d-2cbd-4913-d02c-afbf506df228"
      },
      "source": [
        "model = BertDWF()\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxCvA8GpwsbJ"
      },
      "source": [
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "\n",
        "weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "weights = weights.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 2e-5)  \n",
        "\n",
        "# define the loss function\n",
        "cross_entropy  = nn.NLLLoss(weight=weights) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfnDgXCkiIM8"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw0_W8V1wyim"
      },
      "source": [
        "def train():\n",
        "  model.train()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  total_preds=[]\n",
        "  \n",
        "  for step,batch in enumerate(train_dataloader):\n",
        "    # progress update \n",
        "    if step>0 and step % 100 == 0:\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
        "    batch = [r.to(device) for r in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    model.zero_grad()        \n",
        "\n",
        "    # get model predictions\n",
        "    preds = model(sent_id, mask)\n",
        "    loss = cross_entropy(preds, labels)\n",
        "    total_loss = total_loss + loss.item()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "    preds=preds.detach().cpu().numpy()\n",
        "    total_preds.append(preds)\n",
        "\n",
        "  avg_loss = total_loss / len(train_dataloader)\n",
        "  \n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpqNx2hWw1pB"
      },
      "source": [
        "def evaluate():\n",
        "  print(\"\\nEvaluating...\")\n",
        "  \n",
        "  # deactivate dropout layers\n",
        "  model.eval()\n",
        "  total_loss, total_accuracy = 0, 0\n",
        "  # empty list to save the model predictions\n",
        "  total_preds = []\n",
        "  for step,batch in enumerate(val_dataloader):\n",
        "    # Progress update\n",
        "    if step>0 and step % 100 == 0:      \n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
        "    batch = [t.to(device) for t in batch]\n",
        "    sent_id, mask, labels = batch\n",
        "    # prediction\n",
        "    with torch.no_grad():\n",
        "      preds = model(sent_id, mask)\n",
        "      loss = cross_entropy(preds,labels)\n",
        "      total_loss = total_loss + loss.item()\n",
        "      preds = preds.detach().cpu().numpy()\n",
        "      total_preds.append(preds)\n",
        "  # compute the validation loss\n",
        "  avg_loss = total_loss / len(val_dataloader) \n",
        "  # reshape the predictions\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\n",
        "\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYpo1mrhw6Da",
        "outputId": "eaf1a7c8-14e2-4f7c-fc81-995e295a2e92"
      },
      "source": [
        "# set initial loss to infinite\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# empty lists to store training and validation loss of each epoch\n",
        "train_losses=[]\n",
        "valid_losses=[]\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    print('\\n Epoch {:} / {:}'.format(epoch + 1, config.EPOCHS))\n",
        "    train_loss, _ = train()\n",
        "    valid_loss, _ = evaluate()\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    print(f'\\nTraining Loss: {train_loss:.3f} \\n Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Epoch 1 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.488 \n",
            " Validation Loss: 0.419\n",
            "\n",
            " Epoch 2 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.365 \n",
            " Validation Loss: 0.400\n",
            "\n",
            " Epoch 3 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.287 \n",
            " Validation Loss: 0.457\n",
            "\n",
            " Epoch 4 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.219 \n",
            " Validation Loss: 0.478\n",
            "\n",
            " Epoch 5 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.168 \n",
            " Validation Loss: 0.570\n",
            "\n",
            " Epoch 6 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.138 \n",
            " Validation Loss: 0.632\n",
            "\n",
            " Epoch 7 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.105 \n",
            " Validation Loss: 0.716\n",
            "\n",
            " Epoch 8 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.096 \n",
            " Validation Loss: 0.761\n",
            "\n",
            " Epoch 9 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.077 \n",
            " Validation Loss: 0.895\n",
            "\n",
            " Epoch 10 / 10\n",
            "  Batch   100  of    547.\n",
            "  Batch   200  of    547.\n",
            "  Batch   300  of    547.\n",
            "  Batch   400  of    547.\n",
            "  Batch   500  of    547.\n",
            "\n",
            "Evaluating...\n",
            "  Batch   100  of    118.\n",
            "\n",
            "Training Loss: 0.062 \n",
            " Validation Loss: 0.942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzXSUO12w9DB",
        "outputId": "793de7e5-887f-49e2-c709-3408bdbf1010"
      },
      "source": [
        "#load weights of best model\n",
        "path = 'saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eC08q8AWjwYh"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GS69VMv1-pn"
      },
      "source": [
        "# get predictions for test data\n",
        "with torch.no_grad():\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUgpNlVt2A8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4190011f-f5de-49e5-8d4d-801b34c79b41"
      },
      "source": [
        "preds = np.argmax(preds, axis = 1)\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.78      0.84      2369\n",
            "           1       0.71      0.89      0.79      1480\n",
            "\n",
            "    accuracy                           0.82      3849\n",
            "   macro avg       0.81      0.83      0.82      3849\n",
            "weighted avg       0.84      0.82      0.82      3849\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCMk5kFq2DQW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}